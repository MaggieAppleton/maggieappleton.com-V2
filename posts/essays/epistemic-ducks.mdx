---
title: "Large Language Models as Epistemic Rubber Ducks"
description: "Using large language models for reflective thinking and metacognition, rather than generating facts or final outputs"
startDate: "2022-11-20"
updated: "2022-11-23"
type: "essay"
topics:
  [
    "Machine Learning",
    "Design",
    "Artificial Intelligence",
    "Tools for Thought",
  ]
cover: "/images/covers/rubberduck@2x.png"
growthStage: "budding"
toc: true
aliases: ["epistemic rubber ducks"]
---

<AssumedAudience>People who have read my piece on [[Large Language Models as Failed Oracles]], or who already intimately familiar with the current failures and shortcomings of large language models</AssumedAudience>

<Spacer size="small" />

In [[Language Models as Failed Oracles]] I laid out a series of reasons that our current language models are not the oracles we were promised. Some of the problems will be solved by brute-force scaling compute. Others, such as the lack of stable, situated knowledge from a unified viewpoint, feel inherent to language models.

Until we develop more robust language models and interfaces that are transparent about their reasoning and confidence level, we need to change our framing of them. We should not be thinking and talking about these systems as superintelligent, trustworthy oracles. At least, not right now.

There's a number of interesting framings that could lead us down very different pathways for how these models develop. We could think of them as giant, searchable databases, as research assitants, as xxx.

We should instead think of them as rubber ducks.

<BasicImage src="/images/posts/epistemic-ducks/duck1.jpg" width="550px" margin="2rem auto 0" />

## Epistemic rubber ducking 

Rubber ducking is the practice of having a friend or colleague sit and listen while you work through a problem. They aren't there to tell you the solution to the problem or help actively solve it. They might prompt you with questions and occasionally make affirmational sounds. But their primary job is to help you solve their problem yourself. They're like a rubber duck, quietly listening, while you talk yourself out of a hole. 

The term comes [from programming](https://en.wikipedia.org/wiki/Rubber_duck_debugging) where you're frequently faced with poorly defined problems that require a bit of thinking out loud.<Footnote idName={10}>Originally coined from a story in [The Pragmatic](https://pragprog.com/titles/tpp20/the-pragmatic-programmer-20th-anniversary-edition/) [Programmer](https://pragprog.com/titles/tpp20/the-pragmatic-programmer-20th-anniversary-edition/) where one programmer carried around a literal rubber duck to talk through their code with</Footnote> Simply answering the question "what am I trying to do here?" is often enough to get started on a solution.

**Language models are well suited to rubber ducking.** Their mimicry makes them good reflective thinking partners, not independent sources of truth. 

And not just any rubber ducking...

[decorate the text with floating rubber ducks and sparkles]

<Center><span style={{fontSize: "3rem", color: "goldenrod", marginBottom: "3rem"}}>Epistemic rubber ducking</span></Center>

Epistemology is the study of how we know what we know, also called “theory of knowledge.” It deals with issues like how valid a claim is, how strong its claims and counter-arguments are, whether the evidence came from a reliable source, and whether cognitive biases might be warping our opinions.

Epistemic rubber ducking, then, is talking through an idea, claim, or opinion you hold, with a partner who helps you think through the epistemological dimensions of your thoughts. This isn't simply a devil's advocate incessantly pointing out all the ways you're wrong. <Footnote idName={11}>No one would tolerate that for long, which is why Effective Altruism is such an exhausting community to interact with.</Footnote>

A useful epistemic duck would need to be supportive and helpful. It would need to simply ask questions and suggest ideas, none of which you're required to accept or integrate, but are there if you want them. It could certainly prod and critique, but in a way that helps you understand the other side of the coin, and realise the gaps and flaws in your arguments.

## A collection of speculative prototypes

What would this look like in practice?

[Placeholder images for now - will make these video demos. Likely going to explore more tiny ideas to include]

### Branches

<BasicImage width="800px" src="/images/posts/epistemic-ducks/demo2.jpg" />

### Argument maps

<BasicImage width="800px" src="/images/posts/epistemic-ducks/demo2.jpg" />

### Daemons

<BasicImage width="800px" src="/images/posts/epistemic-ducks/demo1.jpg" />

### Epi

<BasicImage width="800px" src="/images/posts/epistemic-ducks/demo3.jpg" />



---

## From anthropomorphism to animals

There's a side quest I promised myself I wouldn't go down in this piece, but I'll briefly touch on it. I think we should take the **duck-ness** of language models as rubber ducks seriously. Meaning that conceiving of language models as ducks – an animal species with some capacity for intelligence – is currently better than conceiving of them as human-like agents.

I have very different expectations of a duck than I do of a human. I expect it can sense its environment and make decisions that keep it alive and happy and fat. I expect it has a more nuanced understanding of fish species and water currents and migration than I do. I don't expect it would be a very competent babysitter or bus driver or physics teacher.

In short, the duck has very different intellectual and physical capacities from you or I. The same will be true of various “AI” systems like language models. Their form of “thinking” will certainly be more human-ish than duck-ish, but it would be a mistake to expect the same of them as we do of humans.

Kate Darling has [made this argument](https://www.penguin.co.uk/books/309409/the-new-breed-by-darling-kate/9780141988641) around robots: that we should look to our history with animals as a touchstone for navigating our future with robots and AI.

<ResourceBook
  url="https://www.penguin.co.uk/books/309409/the-new-breed-by-darling-kate/9780141988641"
  title="The New Breed: How to Think About Robots"
  author="Kate Darling"
  image="/images/books/newbreed.jpg"
>

Kate Darling argues we should look to our history with animals to help guide our relationship with robots. We've lived alongside many other species in mutually-beneficial relationships for centuries, giving us a long history of social, legal, and emotional insights. Rather than making robots that imitate and replace humans, we can instead develop their skills and abilities to complement our own.

</ResourceBook>

<Spacer size="xs" />

An alternate analogy I've heard floating around is “aliens.” Many AI researchers talk about ML systems as a kind of [alien intelligence](https://www.youtube.com/watch?v=cdiD-9MMpb0&t=391s). Given our cultural narratives around aliens as parasitic killers that are prone to exploding out of your chest, I'm pretty averse to the metaphor. Having an alien intelligence in my system sounds threatening. It certainly doesn't sound like a helpful collaborative thinking partner. <Footnote idName={12}>This might be a good thing if it helps motivate us to take AI alignment seriously.</Footnote>

<TwoColumn maxWidth="950px" alignItems="top">

<BasicImage src="/images/posts/epistemic-ducks/alien1.png" alt="a scary alien" />
<BasicImage src="/images/posts/epistemic-ducks/alien2.png" alt="a scary alien" />

</TwoColumn>

<Subtext>A few friendly, helpful thinking partners – Made with Midjourney</Subtext>

I think there's a lot more to explore here around the metaphors we use to talk about language models and AI systems, but I'll save it for another post.


