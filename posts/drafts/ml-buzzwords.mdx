---
title: "AI and ML Buzzword Bingo"
description: "All the buzzwords I've learned as a new entry into the field of artificial intelligence and machine learning"
updated: "2022-10-30"
startDate: "2022-10-30"
type: "note"
topics: ["Artificial Intelligence", "Machine Learning"]
growthStage: "budding"
toc: true
---

<IntroParagraph>

Two months ago I [joined](/joining-ought) an artificial intelligence and machine learning research lab. This was a bold move considering I haven't been paying much attention to the AI scene.

</IntroParagraph>

I'd been ambiently tracking the developments in [language models]() and the concerns over [existential risk](), but couldn't tell you anything about the specifics of how machine learning worked or who the major players were.

I was immersed in other worlds; [interface design](/patterns), [developer education](https://justjavascript.com/), [better programming interfaces](/programming-pictures), and [tools for thought](/tools-for-thought). Through writing about these topics, I connected with [Ought](https://ought.org) and found we had many of the same goals and values in mind. And so down the AI rabbit hole I went.

It instantly felt like drowning. Every conversation was filled with unfamiliar terms, people, companies, datasets, model names, and arVix papers. This world _loves_ technical neologisms, acronyms, and jargon. A lot of it is justified. We're talking about concepts and techniques no one has ever dealt with before. But it creates a formidable barrier for newcomers.

Here's a list of all the buzzwords and phrases I've learned so far. Most are technical, but some relate to the community and culture around ML/AI. Leaving a trail of breadcrumbs behind me for others to find is the least I can do. If you're new to the AI industry, good luck and godspeed ðŸ«¡

<Spacer size="xs" />

<SimpleCard alignLeft>

### Benchmarks

"Benchmarks are datasets composed of tests and metrics to measure the performance of AI systems on specific tasks. An example is ImageNet, a popular benchmark for evaluating image classification systems"

</SimpleCard>

<SimpleCard alignLeft>

### Decomposition

Breaking models down into smaller, observable steps

</SimpleCard>

<SimpleCard alignLeft>

### Embedding

Lalalal

</SimpleCard>

<SimpleCard alignLeft>

### Ensambling

Using many different models to double-check the answer to a query

</SimpleCard>

<SimpleCard alignLeft>

### Factored Cognition

Lalalal

</SimpleCard>

<SimpleCard alignLeft>

### Fine Tuning

Training models on task-specific datasets. The model updates its weights to perform better on that specific task.

</SimpleCard>


<SimpleCard alignLeft>

### Gradient Descent

Lalalal

</SimpleCard>

<SimpleCard alignLeft>

### Large Language Models

Lalalal

</SimpleCard>

<SimpleCard alignLeft>

### Latent Space

Lalalal

</SimpleCard>

<SimpleCard alignLeft>

### Neural Networks

Lalalal

</SimpleCard>

<SimpleCard alignLeft>

### Perplexity

How surprised the model is by an answer?

</SimpleCard>

<SimpleCard alignLeft>

### Parameters

Frankly, I don't fully understand this one yet. Parameters seem to be variables in a language model that seem to increase with the model size. A higher number of parameters equals better performance. GPT-3 has 175 billion parameters and works better than any other (publicly available) model.

</SimpleCard>

<SimpleCard alignLeft>

### Prompt Engineering

Lalalal

</SimpleCard>

<SimpleCard alignLeft>

### Scale is all you need

This is some kind of mantra for the AI safety community.

</SimpleCard>

<SimpleCard alignLeft>

### Transformers

When we stack a whole bunch of transformers onto one another to check the answer with a variety of different weights and attention.

</SimpleCard>

<SimpleCard alignLeft>

### Zero shot, one shot, and few-shot prompts

The number of examples fed to a model within the prompt. 

</SimpleCard>